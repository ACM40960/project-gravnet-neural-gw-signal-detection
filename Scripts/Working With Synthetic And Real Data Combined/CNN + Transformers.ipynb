{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b085ee-8116-48de-944e-75b9302f5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e6255d-136e-4f64-869c-c1fa225cdafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "signal_real = pd.read_csv(\"./combined-dataset/real-signal-injected.csv\", header=None).astype(float)\n",
    "signal_syn = pd.read_csv(\"./combined-dataset/synthetic-signal-injected.csv\", header=None).astype(float)\n",
    "noise_real = pd.read_csv(\"./combined-dataset/real-noise.csv\", header=None).astype(float)\n",
    "noise_syn = pd.read_csv(\"./combined-dataset/synthetic-noise.csv\", header=None).astype(float)\n",
    "\n",
    "signal_real[\"label\"] = 1\n",
    "signal_syn[\"label\"] = 1\n",
    "noise_real[\"label\"] = 0\n",
    "noise_syn[\"label\"] = 0\n",
    "\n",
    "train_data = pd.concat([signal_real, signal_syn, noise_real, noise_syn], axis = 0)\n",
    "train_data = shuffle(train_data, random_state = 42)\n",
    "\n",
    "X = train_data.iloc[:, :-1].values\n",
    "y = train_data[\"label\"].values.astype(np.float32)\n",
    "\n",
    "noise_all = pd.concat([noise_real, noise_syn], axis = 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(noise_all.iloc[:, :-1])\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X = np.expand_dims(X, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60361dde-b89e-4c9b-8e7f-9ddfa381d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1792/1792 [==============================] - 29s 12ms/step - loss: 0.7064 - accuracy: 0.5041 - auc: 0.5068 - val_loss: 0.6951 - val_accuracy: 0.5000 - val_auc: 0.5243\n",
      "Epoch 2/50\n",
      "1792/1792 [==============================] - 20s 11ms/step - loss: 0.6974 - accuracy: 0.5043 - auc: 0.5081 - val_loss: 0.6932 - val_accuracy: 0.5028 - val_auc: 0.5723\n",
      "Epoch 3/50\n",
      "1792/1792 [==============================] - 19s 10ms/step - loss: 0.6959 - accuracy: 0.5061 - auc: 0.5084 - val_loss: 0.6943 - val_accuracy: 0.4933 - val_auc: 0.4977\n",
      "Epoch 4/50\n",
      "1792/1792 [==============================] - 20s 11ms/step - loss: 0.6964 - accuracy: 0.5016 - auc: 0.5028 - val_loss: 0.6932 - val_accuracy: 0.4997 - val_auc: 0.5098\n",
      "Epoch 5/50\n",
      "1792/1792 [==============================] - 20s 11ms/step - loss: 0.6952 - accuracy: 0.5012 - auc: 0.5019 - val_loss: 0.6930 - val_accuracy: 0.5717 - val_auc: 0.5048\n",
      "Epoch 6/50\n",
      "1792/1792 [==============================] - 21s 12ms/step - loss: 0.6943 - accuracy: 0.5010 - auc: 0.5043 - val_loss: 0.6933 - val_accuracy: 0.5061 - val_auc: 0.5109\n",
      "Epoch 7/50\n",
      "1792/1792 [==============================] - 22s 13ms/step - loss: 0.6943 - accuracy: 0.5006 - auc: 0.5014 - val_loss: 0.6928 - val_accuracy: 0.4992 - val_auc: 0.5480\n",
      "Epoch 8/50\n",
      "1792/1792 [==============================] - 26s 15ms/step - loss: 0.5483 - accuracy: 0.6504 - auc: 0.7477 - val_loss: 0.3984 - val_accuracy: 0.7709 - val_auc: 0.8798\n",
      "Epoch 9/50\n",
      "1792/1792 [==============================] - 77s 43ms/step - loss: 0.4035 - accuracy: 0.7711 - auc: 0.8784 - val_loss: 0.4077 - val_accuracy: 0.7687 - val_auc: 0.8819\n",
      "Epoch 10/50\n",
      "1792/1792 [==============================] - 23s 13ms/step - loss: 0.3878 - accuracy: 0.7783 - auc: 0.8853 - val_loss: 0.4142 - val_accuracy: 0.7573 - val_auc: 0.8860\n",
      "Epoch 11/50\n",
      "1792/1792 [==============================] - 22s 12ms/step - loss: 0.3777 - accuracy: 0.7901 - auc: 0.8944 - val_loss: 0.3916 - val_accuracy: 0.7826 - val_auc: 0.8907\n",
      "Epoch 12/50\n",
      "1792/1792 [==============================] - 23s 13ms/step - loss: 0.3675 - accuracy: 0.7935 - auc: 0.8967 - val_loss: 0.3771 - val_accuracy: 0.7732 - val_auc: 0.8957\n",
      "Epoch 13/50\n",
      "1792/1792 [==============================] - 24s 13ms/step - loss: 0.3565 - accuracy: 0.7985 - auc: 0.9012 - val_loss: 0.4124 - val_accuracy: 0.7648 - val_auc: 0.8973\n",
      "Epoch 14/50\n",
      "1792/1792 [==============================] - 23s 13ms/step - loss: 0.3487 - accuracy: 0.8023 - auc: 0.9056 - val_loss: 0.3574 - val_accuracy: 0.7896 - val_auc: 0.9052\n",
      "Epoch 15/50\n",
      "1792/1792 [==============================] - 22s 12ms/step - loss: 0.3348 - accuracy: 0.8138 - auc: 0.9126 - val_loss: 0.3679 - val_accuracy: 0.7807 - val_auc: 0.9038\n",
      "Epoch 16/50\n",
      "1792/1792 [==============================] - 23s 13ms/step - loss: 0.3322 - accuracy: 0.8183 - auc: 0.9149 - val_loss: 0.3655 - val_accuracy: 0.7762 - val_auc: 0.9087\n",
      "Epoch 17/50\n",
      "1792/1792 [==============================] - 30s 17ms/step - loss: 0.3238 - accuracy: 0.8215 - auc: 0.9195 - val_loss: 0.3428 - val_accuracy: 0.8069 - val_auc: 0.9106\n",
      "Epoch 18/50\n",
      "1792/1792 [==============================] - 19s 11ms/step - loss: 0.3197 - accuracy: 0.8223 - auc: 0.9196 - val_loss: 0.3596 - val_accuracy: 0.8066 - val_auc: 0.9114\n",
      "Epoch 19/50\n",
      "1792/1792 [==============================] - 18s 10ms/step - loss: 0.3167 - accuracy: 0.8274 - auc: 0.9218 - val_loss: 0.3331 - val_accuracy: 0.8103 - val_auc: 0.9156\n",
      "Epoch 20/50\n",
      "1792/1792 [==============================] - 19s 11ms/step - loss: 0.3109 - accuracy: 0.8265 - auc: 0.9237 - val_loss: 0.3469 - val_accuracy: 0.8078 - val_auc: 0.9111\n",
      "Epoch 21/50\n",
      "1792/1792 [==============================] - 17s 10ms/step - loss: 0.3052 - accuracy: 0.8337 - auc: 0.9269 - val_loss: 0.3601 - val_accuracy: 0.7905 - val_auc: 0.9135\n",
      "Epoch 22/50\n",
      "1792/1792 [==============================] - 19s 10ms/step - loss: 0.3033 - accuracy: 0.8332 - auc: 0.9270 - val_loss: 0.3603 - val_accuracy: 0.7877 - val_auc: 0.9088\n",
      "Epoch 23/50\n",
      "1792/1792 [==============================] - 18s 10ms/step - loss: 0.3003 - accuracy: 0.8373 - auc: 0.9294 - val_loss: 0.3556 - val_accuracy: 0.7997 - val_auc: 0.9095\n",
      "Epoch 24/50\n",
      "1792/1792 [==============================] - 19s 11ms/step - loss: 0.2982 - accuracy: 0.8359 - auc: 0.9302 - val_loss: 0.3689 - val_accuracy: 0.8041 - val_auc: 0.9122\n",
      "112/112 [==============================] - 1s 4ms/step\n",
      "ROC AUC: 0.915999276297433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.94      0.83      1792\n",
      "         1.0       0.92      0.68      0.78      1792\n",
      "\n",
      "    accuracy                           0.81      3584\n",
      "   macro avg       0.83      0.81      0.81      3584\n",
      "weighted avg       0.83      0.81      0.81      3584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, LayerNormalization, Conv1D, MaxPooling1D,\n",
    "    GlobalAveragePooling1D, Add, MultiHeadAttention, BatchNormalization\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ========= ✅ 1. Preprocessing =========\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.squeeze(-1))  # -> (N, 16384)\n",
    "X_scaled = X_scaled[..., np.newaxis]            # -> (N, 16384, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ========= ✅ 2. Transformer Block =========\n",
    "def transformer_block(x, head_size=32, num_heads=2, ff_dim=64, dropout=0.1):\n",
    "    # Self-attention\n",
    "    attn = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(x, x)\n",
    "    x = Add()([x, attn])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Feed-forward\n",
    "    ff = Dense(ff_dim, activation='relu')(x)\n",
    "    ff = Dense(x.shape[-1])(ff)\n",
    "    x = Add()([x, ff])\n",
    "    x = LayerNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# ========= ✅ 3. Model Builder =========\n",
    "def build_cnn_transformer(input_shape=(16384, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # --- CNN feature extractor ---\n",
    "    x = Conv1D(32, 16, strides=4, padding='same')(inp)   # (4096, 32)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(pool_size=4)(x)                     # (1024, 32)\n",
    "\n",
    "    x = Conv1D(64, 8, strides=2, padding='same')(x)      # (512, 64)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)                     # (256, 64)\n",
    "\n",
    "    # --- Transformer block ---\n",
    "    x = transformer_block(x, head_size=32, num_heads=2, ff_dim=64, dropout=0.1)\n",
    "\n",
    "    # --- Classification ---\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# ========= ✅ 4. Train =========\n",
    "model = build_cnn_transformer()\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ========= ✅ 5. Evaluate =========\n",
    "y_pred = model.predict(X_val).ravel()\n",
    "print(\"ROC AUC:\", roc_auc_score(y_val, y_pred))\n",
    "print(classification_report(y_val, (y_pred > 0.5).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd320c15-142c-4987-9319-5346ca92fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_cnn_transformer_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7716d9d-cba3-41dd-8edb-51cf8b1e5077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
